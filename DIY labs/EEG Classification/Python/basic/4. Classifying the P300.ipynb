{
 "metadata": {
  "name": "",
  "signature": "sha256:aa136b5940b9005c10a5c7f2641dee75553e12c7e51faae7c572ed35d4778df3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Classifying the P300\n",
      "\n",
      "The first tutorial covered visualizing the P300 potential through an ERP plot. This tutorial covers the classification of the P300 potential. The EEG recording used here is made of a subject that is presented with a screen containing 6 icons. These icons were highlighted one by one. For each trial, each icon was highlighted a total of 10 times. The subject selected one of the icons and mentally counted the number of times the chosen icon was highlighted (which was ofcourse always 10), a task designed to keep him focussed on this icon. Every time the chosen icon, which I will refer to now as the target, was highlighted, a P300 potential occurs in the EEG signal. By determining which of the 6 icons corresponds to the largest P300, we can determine which of the icons was the target. This paradigm is a simple version of the famous P300 speller [1].\n",
      "\n",
      "[1] Farwell, L. A., & Donchin, E. (1988). Talking off the top of your head: toward a mental prosthesis utilizing event-related brain potentials. *Electroencephalography and clinical neurophysiology*, 70(6), 510\u2013523, http://www.ncbi.nlm.nih.gov/pubmed/2461285"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data is stored in my public dropbox account and is 53 Mb in size. The following downloads it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "urllib.urlretrieve('https://dl.dropboxusercontent.com/u/79303435/neuroscience_tutorials/tutorial4-01.mat?dl=1', 'tutorial4-01.mat');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loading the data should look very familiar by now:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.io\n",
      "m = scipy.io.loadmat('tutorial4-01.mat')\n",
      "\n",
      "EEG = m['EEG']\n",
      "channel_names = [s[0][0].encode('utf-8') for s in m['channel_names']]\n",
      "event_onsets = m['event_onsets']\n",
      "event_codes = m['event_codes']\n",
      "targets = m['targets'][0] - 1 # -1 because the original list was 1-6, but numpy indexing is 0-5\n",
      "sample_rate = m['sample_rate'][0][0]\n",
      "\n",
      "ntrials = len(targets)\n",
      "classes = unique(targets)\n",
      "nclasses = len(classes)\n",
      "nrepetitions = event_onsets.shape[1] / nclasses\n",
      "nchannels = len(channel_names)\n",
      "\n",
      "print 'Duration of recording is', EEG.shape[1] / float(sample_rate), 'seconds.'\n",
      "print 'Number of EEG channels:', nchannels\n",
      "print\n",
      "print 'Number of trials:', ntrials\n",
      "print 'Target icon for each trial:', targets\n",
      "print 'Number of icons on the screen:', nclasses\n",
      "print 'Number of times each icon was highlighted:', nrepetitions\n",
      "print 'Shape of event matrix:', event_onsets.shape, 'ntrials x (nclasses * nrepetitions)'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "could not read bytes",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-2ec86684e2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tutorial4-01.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mEEG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EEG'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mchannel_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[0mMR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mmdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatfile_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5.pyc\u001b[0m in \u001b[0;36mget_variables\u001b[1;34m(self, variable_names)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 warnings.warn(\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5.pyc\u001b[0m in \u001b[0;36mread_var_array\u001b[1;34m(self, header, process)\u001b[0m\n\u001b[0;32m    250\u001b[0m            \u001b[1;33m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         '''\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5993)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header (scipy/io/matlab/mio5_utils.c:5374)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_real_complex (scipy/io/matlab/mio5_utils.c:6413)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_numeric (scipy/io/matlab/mio5_utils.c:3882)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/mio5_utils.so\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_element (scipy/io/matlab/mio5_utils.c:3604)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/streams.so\u001b[0m in \u001b[0;36mscipy.io.matlab.streams.ZlibInputStream.read_string (scipy/io/matlab/streams.c:2625)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32m/Users/rodin/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/io/matlab/streams.so\u001b[0m in \u001b[0;36mscipy.io.matlab.streams.ZlibInputStream.read_into (scipy/io/matlab/streams.c:2559)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: could not read bytes"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cutting the data into trials. This time, it becomes a 5 dimensional array. Take a look at the resulting dimensions reading the following description:\n",
      "\n",
      "There are 12 trials. During each of these trials, data was collected for each of the 6 icons on the screen. Each icon was highlighted 10 times. The time-onsets when an icon was highlighted is called an epoch. For each epoch, the time interval 0.1 s *before* the onset until 1 s *after* the onset is extracted (1126 samples). The recording contains 32 channels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "window = [int(-0.1*sample_rate), int(1.0*sample_rate)]\n",
      "nsamples = window[1] - window[0]\n",
      "\n",
      "trials = np.zeros((nchannels, nsamples, nrepetitions, nclasses, ntrials))\n",
      "\n",
      "for trial in range(ntrials):\n",
      "    for cl in classes:\n",
      "        onsets = event_onsets[trial, event_codes[trial,:] == (cl + 1)]\n",
      "        for repetition, onset in enumerate(onsets):\n",
      "            trials[:, :, repetition, cl, trial] = EEG[:, window[0]+onset:window[1]+onset]\n",
      "\n",
      "print 'shape of trial matrix:', trials.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of trial matrix: (32, 1126, 10, 6, 12)\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "During the first tutorial, the EEG signal was already filtered in advance. This data is not, so we do it here. The function below applies a bandpass filter with a passband between 0.5 - 30 Hz. Also, each epoch is baselined. The baseline in this case is the mean EEG voltage starting from 0.1 s before the onset of the epoch until the onset, which we regard as 'resting EEG'. This baseline is substracted from the rest of the epoch, so the 'resing EEG' voltage is 0. Any changes to the resting EEG (such as the P300) as now relative to 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.signal\n",
      "\n",
      "# Design and apply the bandpass filter\n",
      "a, b = scipy.signal.iirfilter(3, [0.5/(sample_rate/2.0), 30/(sample_rate/2.0)])\n",
      "trials_filt = scipy.signal.filtfilt(a, b, trials, axis=1)\n",
      "    \n",
      "# Calculate the baseline amplitude on the first 0.1 seconds (this corresponds to the time interval -0.1 - 0)\n",
      "baseline = mean(trials_filt[:, 0:int(0.1*sample_rate), ...], axis=1)\n",
      "trials_filt = trials_filt - tile(baseline[:, np.newaxis, :, :], (1, nsamples, 1, 1, 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "MemoryError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-235-351fd3823cdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Design and apply the bandpass filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miirfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrials_filt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiltfilt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Calculate the baseline amplitude on the first 0.1 seconds (this corresponds to the time interval -0.1 - 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\signal\\signaltools.pyc\u001b[0m in \u001b[0;36mfiltfilt\u001b[1;34m(b, a, x, axis, padtype, padlen)\u001b[0m\n\u001b[0;32m   1552\u001b[0m     \u001b[1;31m# Create y0 so zi*y0 broadcasts appropriately.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[0my0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1554\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis_reverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m     \u001b[1;31m# Reverse y.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\signal\\signaltools.pyc\u001b[0m in \u001b[0;36mlfilter\u001b[1;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msigtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msigtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linear_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mMemoryError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 235
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we'll be using machine learning, split the data into a train and a test set 50-50, like we did in the previous tutorial:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_split = 0.5\n",
      "ntrain_trials = int(train_split * ntrials)\n",
      "ntest_trials = ntrials - ntrain_trials\n",
      "\n",
      "train = trials_filt[..., :ntrain_trials]\n",
      "train_targets = targets[:ntrain_trials]\n",
      "\n",
      "test = trials_filt[..., ntrain_trials:]\n",
      "test_targets = targets[ntrain_trials:]\n",
      "\n",
      "print 'channels x samples x repetitions x classes x trials'\n",
      "print 'Training data:', train.shape\n",
      "print 'Test data:    ', test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The training data can be simplified a little bit. We don't care any longer which epoch belongs to which icon on the screen. We only care about epochs where the target was highlighted versus epochs where a nontarget was highlighted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_trials = []\n",
      "nontarget_trials = []\n",
      "for trial in range(ntrain_trials):\n",
      "    for cl in range(nclasses):\n",
      "        if cl == train_targets[trial]:\n",
      "            target_trials.append( train[..., cl, trial] )\n",
      "        else:\n",
      "            nontarget_trials.append( train[..., cl, trial] )\n",
      "\n",
      "# The shape of the data is now\n",
      "# trials x channels x samples x repetitions\n",
      "target_trials = array(target_trials)\n",
      "nontarget_trials = array(nontarget_trials)\n",
      "\n",
      "# Rearranging the axes a bit to\n",
      "# channels x samples x repetitions x trials\n",
      "target_trials = target_trials.transpose([1,2,3,0])\n",
      "nontarget_trials = nontarget_trials.transpose([1,2,3,0])\n",
      "\n",
      "print 'channels x samples x repetitions x trials'\n",
      "print target_trials.shape\n",
      "print nontarget_trials.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before attempting classification, it is wise to first visualize the data. We do this in the same manner as during tutorial 1 with an ERP plot. So we bring back the `plot_eeg` function with some small improvements:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.collections import LineCollection\n",
      "\n",
      "def plot_eeg(EEG, vspace=100, color='k'):\n",
      "    '''\n",
      "    Plot the EEG data, stacking the channels horizontally on top of each other.\n",
      "\n",
      "    Arguments:\n",
      "    EEG    - Array (channels x samples) containing the EEG data\n",
      "    vspace - Amount of vertical space to put between the channels (default 100)\n",
      "    color  - Color to draw the EEG in (default black)\n",
      "    '''\n",
      "    nchannels, nsamples = EEG.shape\n",
      "    \n",
      "    bases = vspace * arange(nchannels)\n",
      "    EEG = EEG.T + bases\n",
      "    \n",
      "    # Calculate a timeline in seconds, knowing that the extracted time interval was -0.1 - 1.0 seconds\n",
      "    time = arange(nsamples) / float(sample_rate)\n",
      "    time -= 0.1\n",
      "        \n",
      "    # Plot EEG versus time as a line collection. This is a small improvement from the version in tutorial 1\n",
      "    # and is useful for creating a figure legend later on. By default in a legend, every line gets one entry.\n",
      "    # But in this EEG plot, multiple lines share the same entry, so we use a line collection.\n",
      "    traces = LineCollection([zip(time, EEG[:, channel]) for channel in range(nchannels)], colors=color)\n",
      "    gca().add_collection(traces)\n",
      "\n",
      "    # Set the y limits of the plot to leave some spacing at the top and bottom\n",
      "    ylim(-vspace, nchannels * vspace)\n",
      "    \n",
      "    # Set the x limits\n",
      "    xlim(-0.1, 1.0)\n",
      "    \n",
      "    # Plot a vertical line on t=0\n",
      "    axvline(0, color='k')\n",
      "    \n",
      "    # Add gridlines to the plot\n",
      "    grid(True)\n",
      "    \n",
      "    # Label the axes\n",
      "    xlabel('Time (s)')\n",
      "    ylabel('Channels')\n",
      "    \n",
      "    # The y-ticks are set to the locations of the electrodes. The international 10-20 system defines\n",
      "    # default names for them.\n",
      "    gca().yaxis.set_ticks(bases)\n",
      "    gca().yaxis.set_ticklabels(channel_names)\n",
      "    \n",
      "    # Put a nice title on top of the plot\n",
      "    title('EEG data')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the `plot_eeg` function to plot the ERPs of both classes (targets versus nontargets):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First average over trials, then over repetitions\n",
      "target_erp = mean(mean(target_trials, axis=3), axis=2)\n",
      "nontarget_erp = mean(mean(nontarget_trials, axis=3), axis=2)\n",
      "\n",
      "figure(figsize=(4,8))\n",
      "plot_eeg(target_erp, color='b', vspace=5)\n",
      "plot_eeg(nontarget_erp, color='r', vspace=5)\n",
      "legend(['targets', 'non-targets'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The familiar shape of the P300 is clearly visible on almost every channel.\n",
      "\n",
      "Now for the classification. Classifying the P300 is relatively simple:\n",
      "\n",
      "1. For each trial, average across the repetitions, creating one ERP for each of the 6 classes.\n",
      "1. Select channels which show a strong P300 in the training data (done manually here)\n",
      "1. For each channel, extract the average voltage for 20 time windows.\n",
      "\n",
      "The procedure is implemented in the `extract_features` function below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_features(epoch):\n",
      "    '''\n",
      "    Extract features form an epoch for classification.\n",
      "    \n",
      "    arguments:\n",
      "        epoch - An array (channels x samples x repetitions) containing the epoch to extract features from.\n",
      "    returns:\n",
      "        A flat array containing the features.\n",
      "    '''\n",
      "    \n",
      "    # Collect the features into this list\n",
      "    features = []\n",
      "    \n",
      "    # First average over repetitions\n",
      "    epoch = mean(epoch, axis=2)\n",
      "\n",
      "    # Extract channels of interest\n",
      "    channels_of_interest = ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'P3', 'P4']\n",
      "    #channels_of_interest = channel_names\n",
      "    epoch = epoch[[channel_names.index(ch) for ch in channels_of_interest], :]\n",
      "    \n",
      "    # Finally, take the avarage value for 20 time windows\n",
      "    nwindows = 20\n",
      "    window_length = int(epoch.shape[1] / float(nwindows))\n",
      "    for channel in range(len(channels_of_interest)):\n",
      "        for window in range(nwindows):\n",
      "            feature = mean(epoch[channel, window*window_length:(window+1)*window_length])\n",
      "            features.append(feature)\n",
      "    \n",
      "    return array(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Applying the `extract_features` function to create the final training data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_features = vstack([extract_features(target_trials[...,i]) for i in range(target_trials.shape[-1])])\n",
      "nontarget_features = vstack([extract_features(nontarget_trials[...,i]) for i in range(nontarget_trials.shape[-1])])\n",
      "\n",
      "print 'observations x features'\n",
      "print target_features.shape\n",
      "print nontarget_features.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a classifier, we bring back the LDA used in the previous tutorial:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_lda(class1, class2):\n",
      "    '''\n",
      "    Trains the LDA algorithm.\n",
      "    arguments:\n",
      "        class1 - An array (observations x features) for class 1\n",
      "        class2 - An array (observations x features) for class 2\n",
      "    returns:\n",
      "        The projection matrix W\n",
      "        The offset b\n",
      "    '''\n",
      "    nclasses = 2\n",
      "    \n",
      "    nclass1 = class1.shape[0]\n",
      "    nclass2 = class2.shape[0]\n",
      "    \n",
      "    # Class priors: in this case, there are an unequal number of training\n",
      "    # examples for each class. There are 5 times as many nontarget trials\n",
      "    # as target trials.\n",
      "    prior1 = nclass1 / float(nclass1 + nclass2)\n",
      "    prior2 = nclass2 / float(nclass1 + nclass2)\n",
      "    \n",
      "    mean1 = np.mean(class1, axis=0)\n",
      "    mean2 = np.mean(class2, axis=0)\n",
      "    \n",
      "    class1_centered = class1 - mean1\n",
      "    class2_centered = class2 - mean2\n",
      "    \n",
      "    # Calculate the covariance between the features\n",
      "    cov1 = class1_centered.T.dot(class1_centered) / (nclass1 - nclasses)\n",
      "    cov2 = class2_centered.T.dot(class2_centered) / (nclass2 - nclasses)\n",
      "     \n",
      "    W = (mean2 - mean1).dot(np.linalg.pinv(prior1*cov1 + prior2*cov2))\n",
      "    b = (prior1*mean1 + prior2*mean2).dot(W)\n",
      "    \n",
      "    return (W, b)\n",
      "\n",
      "def apply_lda(test, W, b):\n",
      "    '''\n",
      "    Applies a previously trained LDA to new data.\n",
      "    arguments:\n",
      "        test - An array (observations x features) containing the data\n",
      "        W    - The project matrix W as calculated by train_lda()\n",
      "        b    - The offsets b as calculated by train_lda()\n",
      "    returns:\n",
      "        A list containing the classification result for each trial\n",
      "    '''\n",
      "    return test.dot(W) - b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below applies the LDA classifier to determine for each trial, which of the 6 icons corresponds to the largest P300 potential:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(trials, W, b):\n",
      "    '''\n",
      "    Apply the LDA classifier to the test trials.\n",
      "\n",
      "    arguments:\n",
      "        trials - An array (channels x samples x repetitions x classes x trials) containing the test trials.\n",
      "        W      - The weights W as returned by train_lda()\n",
      "        b      - The offsets b as returned by train_lda()\n",
      "    returns:\n",
      "        A list containing the predicted target icon for each trial.\n",
      "    '''\n",
      "    \n",
      "    nclasses = trials.shape[3]\n",
      "    ntrials = trials.shape[4]\n",
      "    \n",
      "    predicted_targets = []\n",
      "    for trial in range(ntrials):\n",
      "        # Feature extraction\n",
      "        features = vstack([extract_features(test[:,:,:,cl,trial]) for cl in range(nclasses)])\n",
      "        \n",
      "        # Classification\n",
      "        p = apply_lda(features, W, b)\n",
      "        \n",
      "        # Determine icon with the highest P300\n",
      "        predicted_targets.append( argmin(p) )\n",
      "        \n",
      "    return array(predicted_targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training the classifier on the training data, applying it on the test data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, b = train_lda(target_features, nontarget_features)\n",
      "predicted_targets = classify(test, W, b)\n",
      "\n",
      "print 'Predicted targets:', predicted_targets\n",
      "print 'Real targets:     ', test_targets\n",
      "print 'Accuracy: %.2f' % (len(flatnonzero(predicted_targets == test_targets)) / float(ntest_trials))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You see that with the first 6 trials as training data, we were able to correctly determine the target icon in the 6 remaining trials, using relatively simple techniques."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats\n",
      "\n",
      "def regress(X, y):\n",
      "    # Regress X onto y\n",
      "    W = linalg.pinv(X).dot(y)\n",
      "    \n",
      "    # Calculate residuals\n",
      "    resid = y - X.dot(W)\n",
      "    \n",
      "    # Degrees of freedom in upcomining calculations\n",
      "    # is the number of observations minus the number of features\n",
      "    df = X.shape[0] - X.shape[1]\n",
      "    \n",
      "    # Calculate normalized covariance matrix of X\n",
      "    scale = resid.dot(resid) / df\n",
      "    X_inv = linalg.pinv(X)\n",
      "    cov_X = X_inv.dot(X_inv.T) * scale\n",
      "    \n",
      "    # Calculate t-tests for the weights\n",
      "    tvalues = W / sqrt(diag(cov_X))\n",
      "    pvalues = scipy.stats.betai(0.5*df, 0.5, df / (df + tvalues ** 2))\n",
      "\n",
      "    return W, pvalues\n",
      "\n",
      "allclose(regress(X,y)[1], m.pvalues)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stepwise(class1, class2, p_in=0.1, p_out=0.15):\n",
      "    # Needed later for regression step\n",
      "    X = vstack((class1, class2))\n",
      "    y = hstack((ones(class1.shape[0]), -1*ones(class2.shape[0])))\n",
      "\n",
      "    # Perform t-tests\n",
      "    pvalues = scipy.stats.ttest_ind(class1, class2, axis=0)[1]\n",
      "    \n",
      "    # Sort features by p-value\n",
      "    features = argsort(pvalues)\n",
      "    pvalues = pvalues[features]\n",
      "    \n",
      "    # Keep track of the features included\n",
      "    features_in = array([], dtype=int)\n",
      "    \n",
      "    # Loop over all significant features (p <= p_in)\n",
      "    for feat in features[pvalues <= p_in]:\n",
      "        features_in = np.append(features_in, feat)\n",
      "        \n",
      "        # Regress using included features\n",
      "        coeff_pvalues = regress(X[:, features_in], y)[1]\n",
      "        \n",
      "        # Only keep features with sufficient pvalues in the regression (p <= p_out)\n",
      "        features_in = features_in[coeff_pvalues <= p_out]\n",
      "        \n",
      "    return features_in"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "selected_features = stepwise(target_features, nontarget_features, p_in=0.1, p_out=0.15)\n",
      "print selected_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_SWLDA(trials, selected_features, W, b):\n",
      "    '''\n",
      "    Apply the LDA classifier to the test trials.\n",
      "\n",
      "    arguments:\n",
      "        trials - An array (channels x samples x repetitions x classes x trials) containing the test trials.\n",
      "        W      - The weights W as returned by train_lda()\n",
      "        b      - The offsets b as returned by train_lda()\n",
      "    returns:\n",
      "        A list containing the predicted target icon for each trial.\n",
      "    '''\n",
      "    \n",
      "    nclasses = trials.shape[3]\n",
      "    ntrials = trials.shape[4]\n",
      "    \n",
      "    predicted_targets = []\n",
      "    for trial in range(ntrials):\n",
      "        # Feature extraction\n",
      "        features = vstack([extract_features(test[:,:,:,cl,trial]) for cl in range(nclasses)])\n",
      "        \n",
      "        # Classification\n",
      "        p = apply_lda(features[:, selected_features], W, b)\n",
      "        \n",
      "        # Determine icon with the highest P300\n",
      "        predicted_targets.append( argmin(p) )\n",
      "        \n",
      "    return array(predicted_targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W, b = train_lda(target_features[:, selected_features], nontarget_features[:, selected_features])\n",
      "predicted_targets = classify_SWLDA(test, selected_features, W, b)\n",
      "\n",
      "print 'Predicted targets:', predicted_targets\n",
      "print 'Real targets:     ', test_targets\n",
      "print 'Accuracy: %.2f' % (len(flatnonzero(predicted_targets == test_targets)) / float(ntest_trials))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}