{
 "metadata": {
  "name": "5. Imagined movement with Golem and Psychic"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Imagined movement with Golem and Psychic\n",
      "\n",
      "Congratulations on making it through the previous tutorial. It was a hard one. In this tutorial your life will be made easier by taking a look at some of the functionality in Golem and Psychic regarding imagined movement. You will have noticed the code in the previous tutorial followed a pattern:\n",
      "\n",
      " - first, a function was written performing a signal processing step, such as a bandpass filter or the CSP algorithm.\n",
      " - next, these functions were applied multiple times along the way\n",
      "\n",
      "It follows naturally that you store these functions in a library, since they will come in handy in many occasions. This leads to a multitude of EEG signal processing libraries. In fact, every major lab seems to have it's own library written in their language of choice. Mine happens to be Python, so my code ends up across the Golem and Psychic modules.\n",
      "\n",
      "This tutorial will perform the same operations as the previous one, but this time the functionality of Golem and Psychic will be used."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we load the BCI competition data, assuming it still exists at the location it was downloaded in the previous tutorial. It is stored in a Golem dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reset\n",
      "import golem, psychic\n",
      "import numpy as np\n",
      "import scipy.io\n",
      "\n",
      "m = scipy.io.loadmat('data_set_IV/100Hz/data_set_IVa_al.mat', struct_as_record=True)\n",
      "m2 = scipy.io.loadmat('data_set_IV/true_labels_al.mat')\n",
      "\n",
      "X = m['cnt'].T\n",
      "nchannels, nsamples = X.shape\n",
      "\n",
      "event_onsets = m['mrk']['pos'][0][0][0]\n",
      "event_codes = m2['true_y'][0]\n",
      "Y = np.zeros((1,nsamples))\n",
      "Y[0,event_onsets] = event_codes\n",
      "\n",
      "sample_rate = m['nfo']['fs'][0][0][0][0]\n",
      "I = np.arange(nsamples) / float(sample_rate)\n",
      "\n",
      "feat_lab = [s[0].encode('utf8') for s in m['nfo']['clab'][0][0][0]]\n",
      "\n",
      "d = golem.DataSet(X=X, Y=Y, I=I, feat_lab=feat_lab)\n",
      "print d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 283574 instances, 118 features [118], 1 classes: [283574], extras: []\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember from the 2nd tutorial that this dataset represents 283574 samples of ongoing EEG data. Each sample is called an instance and has 118 features: one for each channel. The 2nd tutorial covers the Golem dataformat in more detail, so check back if this sounds unfamiliar.\n",
      "\n",
      "Like in the 2nd tutorial, the data is sliced into trials by defining a marker dictionary containing class labels for each event code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdict = {1:'right', 2:'foot'}\n",
      "trials = psychic.slice(d, mdict, (int(0.5*sample_rate), int(2.5*sample_rate)))\n",
      "print trials"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 280 instances, 23600 features [118x200], 2 classes: [140, 140], extras: []\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each instance is now a trial, so `trials.X` contains (channels x samples) features. `trials.ndX` is now 3 dimensional: channels x samples x trials. `trials.Y` contains the class labels: there are 140 trials of each class. Again, check back with tutorial 2 if this founds unfamiliar.\n",
      "\n",
      "Much of the functionality in Golem and Psychic comes in the form of 'nodes'. A node is a class with two methods: `train` and `apply`. The `train` method will configure the node based on some sample data. The `apply` method will apply the transformation the node was designed to do on some data.\n",
      "\n",
      "The first step is to use a bandpass filter on the data, filtering the signal in the 8-15Hz range. This is done by constructing a `psychic.nodes.Filter` node, that takes a filter design function as parameter. We use the same filter design function as in the previous tutorial. Next we call the `train` method of the node that uses the filter design function to design the filter. Finally, we can use `apply` to actually filter the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct a filter node, using a filter design function\n",
      "filt = psychic.nodes.Filter(lambda s: scipy.signal.iirfilter(6, [8/(s/2.0), 15/(s/2.0)]))\n",
      "\n",
      "# Use the filter design on the data, the result is stored internally\n",
      "filt.train(trials)\n",
      "\n",
      "# Now the filter is ready for use\n",
      "trials_filt = filt.apply(trials)\n",
      "print trials_filt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 280 instances, 23600 features [118x200], 2 classes: [140, 140], extras: []\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "before moving on, we need to split the data into a train and a test set. Remember that we calculate the CSP and train the classifier on the training data and check the performance on the test data.\n",
      "\n",
      "Golem offers the `golem.cv.strat_splits` function that makes splitting data easy. It will create so called `splits` containing randomly chosen trials, while making sure that each split contains an equal amount of trials for each class. Below, the data is split into two splits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train, test = golem.cv.strat_splits(trials_filt, 2)\n",
      "\n",
      "print train\n",
      "print test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 140 instances, 23600 features [118x200], 2 classes: [70, 70], extras: []\n",
        "DataSet with 140 instances, 23600 features [118x200], 2 classes: [70, 70], extras: []\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that both the train and the test set have an equal number of trials belonging to each class (70).\n",
      "\n",
      "Recall from the previous tutorial the steps we performed to classify the trials:\n",
      "\n",
      " 1. Calculate the CSP and drop everything but the first and last CSP component\n",
      " 2. Calculate the logvar of those components\n",
      " 3. Classify the trial using Linear Discriminant Analysis\n",
      "\n",
      "Nodes can be linked together forming a chain. The code below constructs three nodes that implement the steps above and links them up as a chain:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline = golem.nodes.Chain([\n",
      "    # Only use the 2 most useful CSP components (first and last)\n",
      "    psychic.nodes.spatialfilter.CSP(2), \n",
      "    \n",
      "    # FeatMap applies the given function to the features. We give it a function that calculates the logvar\n",
      "    golem.nodes.FeatMap(lambda x: np.log(np.var(x, axis=1))), # \n",
      "    \n",
      "    # Finally the Linear Discriminant Analysis\n",
      "    golem.nodes.LDA(),\n",
      "])\n",
      "\n",
      "# Train the pipeline using the training data\n",
      "pipeline.train(train)\n",
      "\n",
      "# Apply the pipeline on the test data\n",
      "result = pipeline.apply(test)\n",
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 140 instances, 2 features [2], 2 classes: [70, 70], extras: []\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`result.X` contains the output of the LDA: the predicted class labels. `result.Y` still contains the true class labels. The code below uses this to calculate the accuracy and the confusion matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print golem.perf.accuracy(result)\n",
      "golem.perf.conf_mat(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.957142857143\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([[ 64.,   6.],\n",
        "       [  0.,  70.]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result may vary every time you run the code, since the train and test set are random splits of the data. It should however be similar to the results in the previous tutorial."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}